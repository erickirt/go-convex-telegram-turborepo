# Lightweight LLM Environment Variables

# Model Configuration
MODEL_PATH=./llama-3.2-1b-instruct-q4.gguf
N_CTX=4096
N_THREADS=8
N_GPU_LAYERS=0

# Performance Settings
OMP_NUM_THREADS=8
MKL_NUM_THREADS=8
OPENBLAS_NUM_THREADS=8
NUMEXPR_NUM_THREADS=8
VECLIB_MAXIMUM_THREADS=8

# Service Configuration
PORT=8082
CONVEX_URL=http://localhost:3001

# LangExtract Integration (Docker only)
# Set to 'true' to install LangExtract in the Docker container
# Note: This will increase startup time but enable enhanced RAG processing
INSTALL_LANGEXTRACT=false

# Python Settings
PYTHONUNBUFFERED=1
PYTHONDONTWRITEBYTECODE=1