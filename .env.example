# Centralized Environment Configuration for Telegram Bot + Convex Backend

# =============================================================================
# TELEGRAM BOT CONFIGURATION
# =============================================================================
# Get your bot token from @BotFather on Telegram
TELEGRAM_TOKEN=your_telegram_bot_token_here

# Your bot's username (without @, but including _bot suffix)
# Example: rust_telegram_bot_example_bot
# This is used to construct the bot URL: t.me/{TELEGRAM_BOT_USERNAME}
# Note: Can be left as placeholder if running in web-only mode
TELEGRAM_BOT_USERNAME=not-available-enter-in-env-file

# =============================================================================
# CONVEX BACKEND CONFIGURATION
# =============================================================================
# Backend service ports
NEXT_PUBLIC_CONVEX_PORT=3210
NEXT_PUBLIC_CONVEX_SITE_PROXY_PORT=3211

# Bot should connect to the site proxy port (3211) where API endpoints are available
CONVEX_URL=http://convex-backend:3211
CONVEX_DASHBOARD_PORT=6791

# Instance configuration
# IMPORTANT: Change this secret for production!
CONVEX_INSTANCE_NAME=convex-telegram-bot
CONVEX_INSTANCE_SECRET=0000000000000000000000000000000000000000000000000000000000000000

# Web Dashboard Configuration
WEB_DASHBOARD_PORT=3000

# LLM Service Ports
LIGHTWEIGHT_LLM_PORT=8082
VECTOR_CONVERT_PORT=7999

# =============================================================================
# SYSTEM RESOURCES CONFIGURATION
# =============================================================================
# Total RAM available on the host machine (for display in UI)
NEXT_PUBLIC_RAM_AVAILABLE=8G

# Individual service RAM allocations (used for calculation)
# These values should match the memory limits in docker-compose.yaml
# Optimized for 8GB machine with focus on LLM and vector processing
NEXT_PUBLIC_CONVEX_BACKEND_RAM=1.5G
NEXT_PUBLIC_CONVEX_DASHBOARD_RAM=256M
NEXT_PUBLIC_TELEGRAM_BOT_RAM=128M
NEXT_PUBLIC_VECTOR_CONVERT_LLM_RAM=2G
NEXT_PUBLIC_LIGHTWEIGHT_LLM_RAM=4G
NEXT_PUBLIC_WEB_DASHBOARD_RAM=512M

# Total RAM allocated for the RAG chatbot system (set during setup or calculated dynamically)
# This can be set manually or calculated automatically from individual service allocations
NEXT_PUBLIC_TOTAL_RAM_ALLOCATED=

# Model Configuration (Next.js Public Environment Variables)
# These are exposed to the client-side and displayed in the UI
NEXT_PUBLIC_VECTOR_CONVERT_MODEL="all-MiniLM-L6-v2"
NEXT_PUBLIC_VECTOR_CONVERT_MODEL_HUGGINGFACE_URL="https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2"
NEXT_PUBLIC_LLM_MODEL="Meta Llama 3.2"
NEXT_PUBLIC_LLM_MODEL_HUGGINGFACE_URL="https://huggingface.co/meta-llama/Llama-2-7b-chat-hf"

# Convex URLs for self-hosted deployment
CONVEX_CLOUD_ORIGIN=http://127.0.0.1:3211
CONVEX_SITE_ORIGIN=http://127.0.0.1:3211
NEXT_PUBLIC_DEPLOYMENT_URL=http://127.0.0.1:3211
CONVEX_URL=http://convex-backend:3211

# Development settings
CONVEX_RELEASE_VERSION_DEV=
DO_NOT_REQUIRE_SSL=true
DISABLE_BEACON=true
REDACT_LOGS_TO_CLIENT=false

# Logging
RUST_LOG=info
RUST_BACKTRACE=

# Optional: External database configuration (leave empty to use built-in storage)
DATABASE_URL=
POSTGRES_URL=
MYSQL_URL=

# Optional: GitHub API token for changelog functionality (higher rate limits)
GITHUB_TOKEN=

# Optional: AWS S3 configuration for file storage
AWS_REGION=
AWS_ACCESS_KEY_ID=
AWS_SECRET_ACCESS_KEY=
AWS_SESSION_TOKEN=
S3_STORAGE_EXPORTS_BUCKET=
S3_STORAGE_SNAPSHOT_IMPORTS_BUCKET=
S3_STORAGE_MODULES_BUCKET=
S3_STORAGE_FILES_BUCKET=
S3_STORAGE_SEARCH_BUCKET=
S3_ENDPOINT_URL=